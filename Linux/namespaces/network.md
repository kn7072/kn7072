https://man7.org/linux/man-pages/man8/ip-netns.8.html
https://habr.com/ru/post/549414/

__network namespace__ изолирует ресурсы, связанные с сетью: процесс, работающий в отдельном network namespace, имеет собственные сетевые устройства, таблицы маршрутизации, правила фаервола и т.д. Мы можем непосредственно увидеть это на практике, рассмотрев наше текущее сетевое окружение.

__Команда ip__

Поскольку в этом посте мы будем взаимодействовать с сетевыми устройствами, мы вернем жесткое требование наличия прав суперпользователя, которое мы смягчили в предыдущих постах. С этого момента мы будем предполагать, что как ip, так и isolate будут запускаться с sudo.

__$ ip link list__
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:96:2e:3b brd ff:ff:ff:ff:ff:ff

 (здесь есть lo — loopback-интерфес и `ens33, ethernet-интерфейс LAN.)

Как и со всеми другими пространствами имён, система стартует с начальным network namespace, которому принадлежат все процесс процессы, если не задано иное. Выполнение команды __ip link list__ как есть показывает нам сетевые устройства, принадлежащие изначальному пространству имён (поскольку и наш шелл, и команда ip принадлежат этому пространству имён).

## Именованные пространства имён Network

Давайте создадим новый network namespace:

__$ ip netns add coke__
__$ ip netns list__
coke

И снова мы использовали команду ip. Подкоманда netns позволяет нам играться с пространствами имён network: например, мы можем создавать новые сетевые пространства network с помощью подкоманды add команды netns и использовать list для их вывода.
Вы могли заметить, что list возвращал только наш вновь созданный namespace. Разве он не должен возвращать по крайней мере два, одним из которых был бы исходным namespace, о котором мы упоминали ранее? Причина этого в том, что ip создаёт то, что называется named network namespace, который является просто network namespace, идентифицируемый уникальным именем (в нашем случае coke). Только именованные пространства имён network отображаются подкомандой list, а изначальный network namespace не именованный.

Проще всего получить именованные пространства имён network. Например, в каждом именованном network namespace создаётся файл в каталоге /var/run/netns и им сможет воспользоваться процесс, который хочет переключиться на свой namespace. Другое свойство именованных пространств имён network заключается в том, что они могут существовать без наличия какого-либо процесса — в отличие от неименованных, которые будут удалены как только завершатся все принадлежащие им процессы.

Теперь, когда у нас есть дочерний network namespace, мы можем взглянуть на сеть с его точки зрения.
Мы будем использовать приглашение командной строки C$ для обозначения шелла, работающего в дочернем network namespace.

__$ ip netns exec coke bash__
__C$ ip link list__
1: lo: <LOOPBACK> mtu 65536 qdisc noop __state DOWN__ mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

Запуск субкоманды exec $namespace $command выполняет $command в именованном network namespace $namespace. Здесь мы запустили шелл внутри пространства имён coke и посмотрели доступные сетевые устройства. Мы видим, что, по крайней мере, наше устройство ens33 исчезло. Единственное устройство, которое видно, это лупбек и даже этот интерфейс __погашен__(state DOWN).
__C$ ping 127.0.0.1__
connect: Network is unreachable

Мы должны теперь свыкнуться с тем, что настройки по умолчанию для пространств имён обычно очень строгие. Для пространств имён network, как мы видим, никаких устройств, помимо loopback, не будет доступно. Мы можем поднять интерфейс loopback без всяких формальностей:

__C$ ip link set dev lo up__
__C$ ping 127.0.0.1__
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.034 ms
...

## Сетевое изолирование

Мы уже начинаем понимать, что запустив процесс во вложенном network namespace, таком как coke, мы можем быть уверены, что он изолирован от остальной системы в том, что касается сети. Наш шелл-процесс, работающий в coke, может общаться только через loopback. Это означает, что он может общаться только с процессами, которые также являются членами пространства имён coke, но в настоящее время других таких процессов нет (и, во имя изолированности, мы хотели бы, чтобы так и оставалось), так что он немного одинок. Давайте попробуем несколько ослабить эту изолированность. Мы создадим туннель, через который процессы в coke смогут общаться с процессами в нашем исходном пространстве имён.


Сейчас любое сетевое общение должно происходить через какое-то сетевое устройство, а устройство может существовать ровно в одном network namespace в данный конкретный момент времени, поэтому связь между любыми двумя процессами в разных пространствах имён должна осуществляться как минимум через два сетевых устройства — по одному в каждом network namespace.


## Устройства veth

Для выполнения этого нашего требования, мы будем использовать сетевое устройство __virtual ethernet (или сокращённо veth)__. Устройства veth всегда создаются как пара устройств, связанных по принципу туннеля, так что сообщения, отправленные на одном конце, выходят из устройства на другом. Вы могли бы предположить, что мы могли бы легко иметь один конец в исходном network namespace, а другой — в нашем дочернем network namespace, а всё общение между пространствами имён network проходило бы через соответствующее оконечное устройство veth (и вы были бы правы).

#Создание пары veth (veth0 <=> veth1)
__$ ip link add veth0 type veth peer name veth1__

#Перемещение veth1 в новое пространство имён
__$ ip link set veth1 netns coke__

#Просмотр сетевых устройств в новом пространстве имён
__C$ ip link list__
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
7: veth1@if8: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether ee:16:0c:23:f3:af brd ff:ff:ff:ff:ff:ff link-netnsid 0

Наше устройство veth1 теперь появилось в пространстве имён coke. Но чтобы заставить пару veth работать, нам нужно назначить там IP-адреса и поднять интерфейсы. Мы сделаем это в каждом соответствующем network namespace.    

#В исходном пространстве имён
__$ ip addr add 10.1.1.1/24 dev veth0__
__$ ip link set dev veth0 up__

#В пространстве имён coke
__C$ ip addr add 10.1.1.2/24 dev veth1__
__C$ ip link set dev veth1 up__

C$ ip addr show veth1
7: veth1@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether ee:16:0c:23:f3:af brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.1.1.2/24 scope global veth1
       valid_lft forever preferred_lft forever
    inet6 fe80::ec16:cff:fe23:f3af/64 scope link
       valid_lft forever preferred_lft forever

Мы должны увидеть, что интерфейс veth1 поднят и имеет назначенный нами адрес 10.1.1.2. Тоже самое должно произойти с veth0 в исходном пространстве имён. Теперь у нас должна быть возможность сделать интер-namespace ping между двумя процессами, запущенными в обоих пространствах имён.

__$ ping -I veth0 10.1.1.2__
PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.
64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.041 ms
...
__C$ ping 10.1.1.1__
PING 10.1.1.1 (10.1.1.1) 56(84) bytes of data.
64 bytes from 10.1.1.1: icmp_seq=1 ttl=64 time=0.067 ms

## Немного о Linux namespaces
https://xakep.ru/2018/01/15/vpn-linux-network-namespaces/

В Linux реализована функция пространства имен (namespace), которая отвечает за изоляцию разных ресурсов системы. Она вовсю применяется в проектах контейнеризации, например таких как Docker. Существует несколько типов пространств имен: pid, net, mnt, user, uts, ipc.

Нас интересует пространство имен для сетей (netns), которое изолирует сетевые ресурсы. Для каждого netns можно назначать: свои интерфейсы, наборы IP-адресов и портов (сокеты), таблицы маршрутизации, правила файрвола и так далее. Есть возможность перемещать интерфейсы из одного netns в другой. Физический интерфейс (например, eth0) может находиться одновременно только в одном netns.

Изначально все интерфейсы и процессы содержатся в исходном (initial) netns, у него нет конкретного имени, и он не отображается в списке. После освобождения какого-либо netns все физические интерфейсы, которые были в нем, возвращаются в initial netns. Освобождение происходит после завершения последнего процесса в этом netns. Например, даже если удалить конкретный netns, но процесс, запущенный в нем, будет активным, физический интерфейс не перенесется обратно в исходный netns до тех пор, пока процесс не будет завершен. Рассмотрим этот вариант позже.

## База netns

За контроль netns отвечает утилита ip из пакета iproute2. Для соединения netns между собой можно использовать пару виртуальных интерфейсов veth. Рассмотрим пример создания нескольких netns и их соединения. Для этого выполним

__$ sudo ip netns add ns_1__
__$ sudo ip netns add ns_2__

Наличие этих netns смотри командой __ip netns list__ или просто __ip netns__, так как list — действие по умолчанию. Добавим виртуальную пару при помощи команды

__$ sudo ip link add dev virt01 type veth peer name virt02__
__ip netns list__

Интерфейсы добавились, теперь переместим virt01 в netns ns_1, а virt02 — в ns_2.

__$ sudo ip link set virt01 netns ns_1__
__$ sudo ip link set virt02 netns ns_2__

Если не появилось сообщений об ошибках, значит, все прошло успешно. Для выполнения команды внутри netns используется команда
__$ sudo ip netns exec <имя netns> <команда для выполнения>__

Например, посмотреть список доступных интерфейсов внутри netns ns_1 можно при помощи
__$ sudo ip netns exec ns_1 ip link__

Почитав man, узнаем, что выполнять команды ip внутри netns можно при помощи 
__$ sudo ip -n <имя netns>__, а значит, __$ sudo ip netns exec ns_1 ip link__ заменяем на __$ sudo ip -n ns_1 link__

По умолчанию интерфейсы, созданные или перемещенные в netns, пребывают в отключенном состоянии, даже lo.
Добавим нашим интерфейсам virt01 и virt02 по IP-адресу и переведем их в состояние UP. Для этого воспользуемся «прокачанной» командой, подсмотренной в man:

__$ sudo ip -n ns_1 addr add 10.0.0.1/24 dev virt01__
__$ sudo ip -n ns_2 addr add 10.0.0.2/24 dev virt02__
__$ sudo ip -n ns_1 link set dev virt01 up__
__$ sudo ip -n ns_2 link set dev virt02 up__
__$ sudo ip -n ns_1 addr show__
__$ sudo ip -n ns_2 addr show__

Как я уже говорил, у каждой netns своя таблица маршрутизации, проверим это.
Для проверки связи между ns_1 и ns_2 воспользуемся командой ping.
__$ sudo ip netns exec ns_1 ping -c 4 10.0.0.2__

Как ты помнишь, адрес 10.0.0.2 принадлежит интерфейсу virt02, который находится в ns_2. Похожим способом netns соединяют с физическим eth0.

## Выполнение команд и запуск процессов внутри netns

Как ты помнишь, команды внутри netns выполняются при помощи
__$ sudo ip netns exec <имя netns> <команда>__

Чтобы не писать все это каждый раз, запустим bash!
__$ sudo ip netns exec ns_1 bash__

После этого все команды будут исполняться внутри netns — заодно и проверим ситуацию с файрволом. __Чтобы вернуться, пиши exit или CTRL + D.__
__iptables -nvL__ Список правил пуст
У меня всегда присутствуют правила в файрволе, так что это точно отдельный набор. 😉
__exit__

__ip link__ отображается enp0s3
Перейдем к примеру с возвращением физического интерфейса в initial netns. Для этого проверим текущее расположение enp0s3 и перенесем его в ns_2.
__$ sudo ip link set dev enp0s3 netns ns_2__
__ip link__  НЕ отображается enp0s3

Поднимем его в netns ns_2 и запустим WireShark от имени пользователя (eakj) там же.
__$ sudo ip -n ns_2 link set dev enp0s3 up__
__$ sudo ip -n ns_2 link__
__$ sudo ip netns exec ns_2 sudo -u eakj wireshark 2>/dev/null &__

Теперь удалим netns ns_2 и посмотрим, вернется ли enp0s3 в initial netns:
__$ sudo ip netns del ns_2__
__ip link__  НЕ отображается enp0s3(enp0s3 не освободится, пока процесс WireShark не будет завершен, даже несмотря на удаление ns_2)
Закроем WireShark и посмотрим, даст ли это результат.
__ip link__ отображается enp0s3